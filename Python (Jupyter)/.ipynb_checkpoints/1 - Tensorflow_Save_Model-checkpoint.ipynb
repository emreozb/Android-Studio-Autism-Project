{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "import numpy as np  \n",
    "from sklearn.model_selection import train_test_split \n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "#Group 2\n",
    "#SW550\n",
    "\n",
    "#data= pd.read_excel('Autism_Data_Child_Version-2_Clean.csv')\n",
    "dataFileName = 'Autism_Data_Child_Version-2 - Clean.csv';\n",
    "data= pd.read_csv(dataFileName)\n",
    "#data.rename(columns= {'austim':'autism'},inplace = True)\n",
    "#data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map Data to numeric values\n",
    "data= data.fillna(method='ffill')\n",
    "data['Sex']=data['Sex'].map({'m': 0, 'f': 1})\n",
    "data['Ethnicity']=data['Ethnicity'].map({\n",
    "    'aboriginal': 0,\n",
    "    'asian': 1,\n",
    "    'black': 2, \n",
    "    'hispanic': 3,\n",
    "    'latino': 4,\n",
    "    'middle eastern': 5,\n",
    "    'south asians': 6,\n",
    "    'white': 7,\n",
    "    'others': 8\n",
    "})\n",
    "data['Ethnicity'].fillna(9, inplace = True)\n",
    "\n",
    "data['Jaundice']=data['Jaundice'].map({'no': 0, 'yes': 1})\n",
    "data['Family_ASD']=data['Family_ASD'].map({'no': 0, 'yes': 1})\n",
    "data['Residence']=data['Residence'].map({\n",
    "    'Afghanistan': 0,\n",
    "    'Albania': 1,\n",
    "    'Algeria': 2,\n",
    "    'American Samoa': 3,\n",
    "    'Angola': 4,\n",
    "    'Argentina': 5,\n",
    "    'Armenia': 6,\n",
    "    'Australia': 7,\n",
    "    'Austria': 8,\n",
    "    'Bahamas': 9,\n",
    "    'Bahrain': 10,\n",
    "    'Bangladesh': 11,\n",
    "    'Belgium': 12,\n",
    "    'Belize': 13,\n",
    "    'Bhutan': 14,\n",
    "    'Bolivia': 15,\n",
    "    'Brazil': 16,\n",
    "    'British Indian Ocean Territory': 17,\n",
    "    'Bulgaria': 18,\n",
    "    'Canada': 19,\n",
    "    'Chile': 20,\n",
    "    'China': 21,\n",
    "    'Costa Rica': 22,\n",
    "    'Dominican Republic': 23,\n",
    "    'Egypt': 24,\n",
    "    'Europe': 25,\n",
    "    'Finland': 26,\n",
    "    'Georgia': 27,\n",
    "    'Germany': 28,\n",
    "    'Ghana': 29,\n",
    "    'Hungary': 30,\n",
    "    'India': 31,\n",
    "    'Iran': 32,\n",
    "    'Iraq': 33,\n",
    "    'Ireland': 34,\n",
    "    'Isle of Man': 35,\n",
    "    'Italy': 36,\n",
    "    'Jamaica': 37,\n",
    "    'Japan': 38,\n",
    "    'Jordan': 39,\n",
    "    'Kuwait': 40,\n",
    "    'Latvia': 41,\n",
    "    'Lebanon': 42,\n",
    "    'Libya': 43,\n",
    "    'Malaysia': 44,\n",
    "    'Malta': 45,\n",
    "    'Mexico': 46,\n",
    "    'Nepal': 47,\n",
    "    'Netherlands': 48,\n",
    "    'New Zealand': 49,\n",
    "    'Nigeria': 50,\n",
    "    'Oman': 51,\n",
    "    'Pakistan': 52,\n",
    "    'Peru': 53,\n",
    "    'Philippines': 54,\n",
    "    'Qatar': 55,\n",
    "    'Romania': 56,\n",
    "    'Russia': 57,\n",
    "    'Saudi Arabia': 58,\n",
    "    'South Africa': 59,\n",
    "    'South Korea': 60,\n",
    "    'Sri Lanka': 61,\n",
    "    'Sudan': 62,\n",
    "    'Sweden': 63,\n",
    "    'Syria': 64,\n",
    "    'Turkey': 65,\n",
    "    'U.S. Outlying Islands': 66,\n",
    "    'United Arab Emirates': 67,\n",
    "    'United Kingdom': 68,\n",
    "    'United States': 69,\n",
    "    'Uzbekistan': 70,\n",
    "    'Virgin Islands, British': 71\n",
    "})\n",
    "data['Used_App_Before']=data['Used_App_Before'].map({'no': 0, 'yes': 1})\n",
    "data['Language']=data['Language'].map({\n",
    "    'arabic': 0,\n",
    "    'english': 1,\n",
    "    'french': 2,\n",
    "    'mandarin': 3,\n",
    "    'portuguese': 4,\n",
    "    'russian': 5,\n",
    "    'spanish': 6,\n",
    "    'swahili': 7,\n",
    "    'turkish': 8,\n",
    "    'urdu': 9\n",
    "})\n",
    "data['User']=data['User'].map({\n",
    "    'friend': 0,\n",
    "    'health care professional': 1,\n",
    "    'parent': 2,\n",
    "    'relative': 3,\n",
    "    'self': 4,\n",
    "    'teacher': 5\n",
    "})\n",
    "data['Class']=data['Class'].map({'NO': 0, 'YES': 1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.head()\n",
    "# Export cleaned data to CSV to compare\n",
    "df = pd.DataFrame(data, columns= [\n",
    "    'A1',\n",
    "    'A2',\n",
    "    'A3',\n",
    "    'A4',\n",
    "    'A5',\n",
    "    'A6',\n",
    "    'A7',\n",
    "    'A8',\n",
    "    'A9',\n",
    "    'A10',\n",
    "    'Age',\n",
    "    'Sex',\n",
    "    'Ethnicity',\n",
    "    'Jaundice',\n",
    "    'Family_ASD',\n",
    "    'Residence',\n",
    "    'Used_App_Before',\n",
    "    'Language',\n",
    "    'User',\n",
    "    'Class'\n",
    "])\n",
    "df.to_csv(r'mappedData.csv', index = False, header = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data.iloc[:,0:18].values\n",
    "y = data.iloc[:,19].values\n",
    "\n",
    "#print(x)\n",
    "#print(y)\n",
    "\n",
    "x_train, x_test,y_train, y_test = train_test_split(x, y,train_size=0.8)\n",
    "\n",
    "#print(x_train.shape,y_train.shape)\n",
    "#print(x_test.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(407, 18) (102, 18)\n"
     ]
    }
   ],
   "source": [
    "#mnist = tf.keras.datasets.mnist\n",
    "#(x_train,y_train),(x_test,y_test) = mnist.load_data()\n",
    "\n",
    "x_train= tf.keras.utils.normalize(x_train,axis=1)\n",
    "x_test= tf.keras.utils.normalize(x_test,axis=1)\n",
    "\n",
    "print (x_train.shape,x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected dense_input to have 3 dimensions, but got array with shape (407, 18)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-8042a3af304c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Users\\Lucian-PC\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 728\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    729\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mD:\\Users\\Lucian-PC\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[0;32m    222\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m           distribution_strategy=strategy)\n\u001b[0m\u001b[0;32m    225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data_adapter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\Lucian-PC\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_process_training_inputs\u001b[1;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, steps_per_epoch, validation_split, validation_data, validation_steps, shuffle, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    545\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    548\u001b[0m     \u001b[0mval_adapter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\Lucian-PC\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[1;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    592\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m         \u001b[0mcheck_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 594\u001b[1;33m         steps=steps)\n\u001b[0m\u001b[0;32m    595\u001b[0m   adapter = adapter_cls(\n\u001b[0;32m    596\u001b[0m       \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\Lucian-PC\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[0;32m   2470\u001b[0m           \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2471\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2472\u001b[1;33m           exception_prefix='input')\n\u001b[0m\u001b[0;32m   2473\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2474\u001b[0m     \u001b[1;31m# Get typespecs for the input data and sanitize it if necessary.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\Lucian-PC\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    563\u001b[0m                            \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    564\u001b[0m                            \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' dimensions, but got array '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 565\u001b[1;33m                            'with shape ' + str(data_shape))\n\u001b[0m\u001b[0;32m    566\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m           \u001b[0mdata_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected dense_input to have 3 dimensions, but got array with shape (407, 18)"
     ]
    }
   ],
   "source": [
    "# Create model and fit data to it\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Dense(32, input_shape=(18,), activation = tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(64, activation = tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(1,activation = tf.nn.sigmoid))\n",
    "\n",
    "model.compile(optimizer = 'adam',loss = 'binary_crossentropy',metrics=['accuracy'])\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train,y_train,epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val_loss, val_acc = model.evaluate(x_test,y_test)\n",
    "print(val_loss,val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('autism_screener')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = tf.keras.models.load_model('autism_screener')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions = new_model.predict([x_test])\n",
    "\n",
    "print(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
